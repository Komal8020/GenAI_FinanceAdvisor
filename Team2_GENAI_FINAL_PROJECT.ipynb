{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wHIB_FQU3-d64ajdIz1gvTxcyS7RfKhN",
      "authorship_tag": "ABX9TyOzkuUN/LLYTAyqU6jB01XY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Komal8020/GenAI_FinanceAdvisor/blob/main/Team2_GENAI_FINAL_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required Python packages using pip\n",
        "# The -q flag suppresses the output to keep the installation process quiet\n",
        "!pip -q install google-generativeai gradio"
      ],
      "metadata": {
        "id": "r6kJ4kGZG9q6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13672940-64a2-4854-8da7-dfb122b67614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import os  # for interacting with the operating system, e.g., file paths\n",
        "from openai import OpenAI  # to interact with OpenAI's API\n",
        "import google.generativeai as genai  # to interact with Google's generative AI API\n",
        "import gradio as gr  # for building a user\n"
      ],
      "metadata": {
        "id": "Hg4PPyGfGvXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up Google API key to authenticate with Google Generative AI\n",
        "GOOGLE_API_KEY = \"AIzaSyDTcCcbzuJkfxIL6I_dvjLu7GmvbPFwttA\"  # Replace with your own API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)  # Configure Google Generative AI with the provided API key"
      ],
      "metadata": {
        "id": "v_iA-U8xHMz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a system prompt for the AI model, outlining its role and response behavior\n",
        "system_prompt = (\n",
        "    \"You are a financial expert who guides people in managing their finances and helps them in budgeting and planning their expenses, \"\n",
        "    \"so people can keep track of their finances. If you are unsure about the information, just say 'I am unsure' \"\n",
        "    \"Also, take feedback for the planning system provided. Ask the user if they want further suggestions.\"\n",
        ")\n",
        "# The system prompt sets the tone and behavior of the AI, instructing it to focus on finance, be transparent when unsure,\n",
        "# and encourage user feedback and further interaction."
      ],
      "metadata": {
        "id": "KPuyF77XHXZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to send a prompt to the Gemini model and receive a response\n",
        "def message_gemini(prompt):\n",
        "    # Prepare the messages with roles (system and user) for the prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},  # System message to set context for the AI\n",
        "        {\"role\": \"user\", \"content\": prompt}  # User's prompt, which is the input query\n",
        "    ]\n",
        "\n",
        "    # Initialize an empty string to accumulate the full prompt\n",
        "    full_prompt = \"\"\n",
        "\n",
        "    # Loop through each message in the messages list to construct the full prompt\n",
        "    for msg in messages:\n",
        "        role = msg[\"role\"]  # Get the role (either system or user)\n",
        "        content = msg[\"content\"]  # Get the content of the message\n",
        "\n",
        "        # Append the system message directly\n",
        "        if role == \"system\":\n",
        "            full_prompt += f\"{content}\\n\"\n",
        "        # Format user messages with a \"User:\" prefix\n",
        "        elif role == \"user\":\n",
        "            full_prompt += f\"User: {content}\\n\"\n",
        "\n",
        "    # Initialize the model using the specified model name \"gemini-2.0-flash-exp\"\n",
        "    model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
        "\n",
        "    # Generate content using the full_prompt as input\n",
        "    response = model.generate_content(full_prompt)\n",
        "\n",
        "    # Return the generated response text from the model\n",
        "    return response.text  # The generated content (response) from Gemini AI model\n"
      ],
      "metadata": {
        "id": "Ua6v0SqzHc5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the message_gemini function with the user input \"Hello\"\n",
        "# The function constructs a full prompt with the system context and the user message (\"Hello\")\n",
        "# It then sends this prompt to the Gemini model and retrieves the AI-generated response.\n",
        "response = message_gemini(\"Hello\")\n",
        "response\n",
        "# The response variable will contain the AI's generated response to the \"Hello\" message.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "OI1D-hyHHi6t",
        "outputId": "134da4e4-bbf7-41ac-e2e4-ddc9caf4f44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! I'm happy to help you get a better handle on your finances. I can assist with budgeting, expense tracking, and financial planning.\\n\\nTo get started, could you tell me a little bit about what you're hoping to achieve? For example:\\n\\n*   Are you looking to create a budget for the first time?\\n*   Do you want to improve your existing budget?\\n*   Are you trying to save for a specific goal (e.g., a down payment on a house, retirement, paying off debt)?\\n*   Do you just want a better way to track where your money is going?\\n\\nThe more information you give me, the better I can tailor my advice to your specific needs. Let's work together to create a plan that works for you.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to stream the response from the Gemini API as the content is generated\n",
        "def stream_gemini(prompt):\n",
        "    # Define the messages to be sent to the Gemini model, including system context and user input\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},  # System message setting the behavior of the model\n",
        "        {\"role\": \"user\", \"content\": prompt},  # User's input query that will be processed by the model\n",
        "    ]\n",
        "\n",
        "    # Initialize the OpenAI API client for Google Generative AI with the necessary API key and endpoint\n",
        "    gemini_api = OpenAI(\n",
        "        api_key=GOOGLE_API_KEY,  # API key for authentication\n",
        "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",  # API endpoint URL\n",
        "    )\n",
        "\n",
        "    # Request a streaming completion from the Gemini model\n",
        "    response = gemini_api.chat.completions.create(\n",
        "        model=\"gemini-2.0-flash-exp\",  # Specify the model to be used\n",
        "        messages=messages,  # Pass the constructed messages to the API\n",
        "        stream=True,  # Enable streaming for real-time response generation\n",
        "    )\n",
        "\n",
        "    # Initialize an empty string to accumulate the results as chunks are received\n",
        "    result = \"\"\n",
        "\n",
        "    # Loop through each chunk in the streaming response\n",
        "    for chunk in response:\n",
        "        # Extract the content from the chunk's choice and append it to the result\n",
        "        result += chunk.choices[0].delta.content or \"\"  # If there's no content, it appends an empty string\n",
        "        # Yield the accumulated result so that it can be processed incrementally\n",
        "        yield result\n"
      ],
      "metadata": {
        "id": "Tfjupi5OHlYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ Gradio UI setup for the financial advisor\n",
        "\n",
        "# Create a Gradio interface to interact with the stream_gemini function\n",
        "with gr.Interface(\n",
        "    fn=stream_gemini,  # Function to be called when user interacts with the UI (streaming response from Gemini)\n",
        "    inputs=[gr.Textbox(label=\"Your Response:\")],  # Textbox input for the user to enter their response/query\n",
        "    outputs=[gr.Markdown(label=\"Response:\")],  # Markdown output area to display the generated response\n",
        "    allow_flagging=\"never\"  # Disable the flagging feature in the UI (users can't flag content)\n",
        ") as view:\n",
        "    # Launch the Gradio interface to make it accessible to the user\n",
        "    view.launch()  # This will start the UI and allow the user to interact with it\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "lv94LouPI3ZM",
        "outputId": "06f28745-c391-47e1-97d8-99ab7cce4839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://584c396238f82761be.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://584c396238f82761be.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle a chat session with Gemini, including maintaining conversation history\n",
        "def chat_gemini(message, history):\n",
        "    # Construct the list of messages to send to the Gemini model\n",
        "    # Combine the system prompt, previous chat history, and the current user message\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    # Initialize the OpenAI API client with the provided API key and endpoint\n",
        "    gemini_api = OpenAI(\n",
        "        api_key=GOOGLE_API_KEY,  # Authentication using the Google API key\n",
        "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",  # The API endpoint URL\n",
        "    )\n",
        "\n",
        "    # Make a streaming request to the Gemini model for chat completion\n",
        "    stream = gemini_api.chat.completions.create(\n",
        "        model=\"gemini-2.0-flash-exp\",  # Specify the model to use for generating the response\n",
        "        messages=messages,  # Pass the combined messages (including history and current user input)\n",
        "        stream=True  # Enable streaming to get the response incrementally\n",
        "    )\n",
        "\n",
        "    # Initialize an empty string to accumulate the response as it streams in\n",
        "    response = \"\"\n",
        "\n",
        "    # Loop through each chunk of the streamed response\n",
        "    for chunk in stream:\n",
        "        # Append the chunk's content to the response string (if any content is present)\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "\n",
        "        # Yield the current response (this allows the response to be processed incrementally)\n",
        "        yield response\n"
      ],
      "metadata": {
        "id": "4IzrE8QbLxch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Chat Interface for interacting with the Gemini model\n",
        "\n",
        "gr.ChatInterface(\n",
        "    fn=chat_gemini,  # The function that handles the chat, which is 'chat_gemini'\n",
        "    type=\"messages\",  # Type of interface to be used - in this case, a message-based chat\n",
        "    # Adding the heading as a title to the interface\n",
        "    title=\"Financial Advisor AI\",  # The title displayed at the top of the Gradio interface\n",
        ").launch(share=True)  # Launch the interface with sharing enabled (makes the app publicly accessible)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "wc0yLQnqfDyu",
        "outputId": "fc21630d-3ccb-4090-b852-4feacb51fe89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8da6f10a7d123ac8b7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8da6f10a7d123ac8b7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R1A6pj08wvqI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}